# from spotify_skip_prediction.datahandler.autoencoder_data_loaders import get_autoencoder_dataloaders
import logging
import numpy as np
import pandas as pd
import torch
from numpy.lib.shape_base import split
from sklearn.preprocessing import StandardScaler

LOG = logging.getLogger(__name__)

def get_rnn_dataloaders(encoded_data, dataset_type, sess_length = 20, feature_width = 4):

    """
    Inputs: encoded data from the autoencoder as a 2D torch tensor with song as dim 1, features as dim 2; filepath to sesssion_lengths.csv
    Outputs: 3D numpy tensor of data split by session_id with -1 appended to end for shorter encoded_datasets
    Read with torch.load()
        dim 1: batches of listening sessions
        dim 2: each song in a session
        dim 3: features of that song
    """

    # encoded_data = pd.read_csv(encoded_data, header=None)                             input is a pytorch tensor, cannot pass tensor to read_csv
    # scale the data
    # scaler_features = StandardScaler(with_mean=False, with_std=True)

    # scaler_features.fit(encoded_data)                                                   # don't think scaling is neccessary here, the inputs to the autoencoder are already scaled, meaning the outputs should as well

    # encoded_data = scaler_features.transform(encoded_data)

    # encoded_data = scaler_features.transform(encoded_data)

    #reshape data as specified in docstring
    encoded_data = encoded_data.reshape(1, -1)
    encoded_data = encoded_data.squeeze()
    encoded_data = encoded_data.reshape(-1, sess_length, feature_width)            # RuntimeError: shape '[-1, 20, 4]' is invalid for input of size 3760484. I recall running into a similar problem, see tests/test_core/test_gym.py/test_trainer_timeseries_regression()

    output = encoded_data

    # begin region
    """
    # snippet to save a 3d tensor with numpy from https://stackoverflow.com/a/3685339, but this is just for us humans to look at. Use torch.save on line 61 for saving to file.
    print(output)
    with open("../../data/encoded_features_"+dataset_type+".csv", 'w') as outfile:
    # I'm writing a header here just for the sake of readability
    # Any line starting with "#" will be ignored by numpy.loadtxt
        outfile.write('# Array shape: {0}\n'.format(output.shape))
        
        # Iterating through a ndimensional array produces slices along
        # the last axis. This is equivalent to data[i,:,:] in this case
        for data_slice in output:

            # The formatting string indicates that I'm writing out
            # the values in left-justified columns 7 characters in width
            # with 2 decimal places.  
            np.savetxt(outfile, data_slice, fmt='%-7.2f')

            # Writing out a break to indicate different slices...
            outfile.write('# New slice\n')
    #end region
    """
    filename = "../../data/encoded_features_" + dataset_type + ".tensor"
    torch.save(output, filename)
    return output

def read_rnn_dataloaders(features, labels, dataset_type, batch_size=20):
    """
    inputs:
    features: encoded data generated by get_rnn_dataloaders
    labels:   labels data generated by get_autoencoder_dataloader
    dataset_type: one of train, test, valid

    outputs:
    dataloader to be passed into rnn
    """

    LOG.info(f"Features {dataset_type} set is {features}")
    LOG.info(f"Labels {dataset_type}set is {labels}")

    # read files to pandas then numpy
    features = torch.tensor((torch.load(features)))
    labels = (pd.read_csv(labels, header=None)).to_numpy()
    # region dataloaders
    LOG.info("Creating dataloaders")
    labels = torch.tensor(labels, dtype=torch.float)

    print(labels.size())
    labels=labels.squeeze()
    labels=labels.squeeze()
    labels=labels.reshape(-1, batch_size, 1)
    #concatenated=torch.cat((features,labels), dim=2)
    #print(features)
    print(features.size())
    print(labels)
    print(labels.size())
    #print(concatenated)


    dataset = torch.utils.data.TensorDataset(
        features,
        labels
        )
    LOG.debug(f"Dataset {dataset_type}:\n{dataset}")


    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=True,
        pin_memory=True,
    )
    # endregion

    return dataloader

if __name__ == "__main__":

    get_rnn_dataloaders("../../data/features_test.csv", "test")

    read_rnn_dataloaders("../../data/encoded_features_test.tensor", "../../data/labels_test.csv", "test")
